---
title: "The Mission"
description: ""
lead: ""
date: 2020-10-13T15:21:01+02:00
lastmod: 2020-10-13T15:21:01+02:00
draft: false
images: []
menu:
  docs:
    parent: "about"
weight: 100
toc: true
---
BigCode is an open scientific collaboration working on the responsible development of large language models for code, empowering the machine learning and open source communities through open governance.

Code LLMs enable the completion and synthesis of code, both from other code snippets and natural language descriptions, and work across a wide range of domains, tasks, and programming languages. These models can, for example, assist professional and citizen developers with building new applications. 

BigCode [invites AI researchers]({{< relref "docs/about/join.md" >}}) to work together on the development of state-of-the-art code LLMs, and collaborate on research topics such as:
- Constructing a representative evaluation suite for code LLMs,  covering a diverse set of tasks and programming languages
- Developing new methods for faster training and inference of  LLMs
- The legal, ethics, and governance aspects of code LLMs

The BigCode project is conducted in the spirit of Open Science.  Datasets, models, and experiments are developed through open collaboration and [released with permissive licenses back to the community]({{< relref "docs/about/ip.md" >}}). While the project has corporate support from ServiceNow and HuggingFace e.g. for hosting models and datasets, and for training compute; all technical governance takes place within working groups and task forces across the community. 

As code LLMs are developed with data from the open-source community, we believe open governance can help to ensure that these models are benefiting the larger developer community. We are working on tools [to give code creators agency over whether their source code is included in the training data](https://huggingface.co/spaces/bigcode/in-the-stack), and give attribution to developers when models output near-copies of the training data. 


<!-- 
Large Language Models (LLMs) are fast becoming an essential tool across many fields of AI research. One striking feature of these large pre-trained models is that they can be adapted to a wide variety of language tasks, often with very little in-domain data. 

BigCode is focused on developing state-of-the-art LLMs for code. Code LLMs enable the completion and synthesis of code, both from other code snippets and natural language descriptions, and work across a wide range of domains, tasks, and programming languages. These models can, for example, assist professional and citizen developers with developing new applications.  

BigCode [invites AI researchers]({{< relref "docs/about/join.md" >}}) to collaborate on the following topics:
- A representative evaluation suite for code LLMs, covering a diverse set of tasks and programming languages
- Responsible data governance for code LLMs
- Faster training and inference methods for LLMs

The project is inspired by [BigScience](https://bigscience.huggingface.co/), an open scientific collaboration which culminated in July 2022 with the release of the [World’s Largest Open Multilingual Language Model](https://huggingface.co/bigscience/bloom). BigCode aims to follow, as well as establish, new responsible AI practices to train and share large language models. We welcome contributions from AI researchers and strive for openness and transparency in the LLM development process. To foster open collaboration across institutes and corporations, we've established a legal framework for the project. See [how we manage intellectual property]({{< relref "docs/about/ip.md" >}}).  -->



<!-- However, it is increasingly difficult for academia and smaller industry labs to develop such models due to their high training cost and large engineering efforts. To democratize the training of LLMs, the [Big Science project](https://bigscience.huggingface.co/) was launched in May 2021 and their year-long program culminated with the release of the [World’s Largest Open Multilingual Language Model](https://huggingface.co/bigscience/bloom). 

Building on this success, we introduce Big Code: an open-scientific collaboration focused on LLMs for code. The purpose of the Big Code project is to collaboratively work towards exploring, training, releasing LLMs for code. To this end, the collaboration [actively seeks contributions from AI practitioners](/docs/about/join) who are interested in the following research topics:
- Curating training datasets for code LLMs
- Distributed training methods for LLMs
- Developing a representative evaluation suite for code LLMs (i.e., covering multiple tasks and programming languages)
- Developing methods for faster training and inference of LLMs
- Discussing the legal aspects of Code LLMs -->